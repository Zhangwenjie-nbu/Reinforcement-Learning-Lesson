# 第 7 课（7.1）：最优价值 (V^*) 与最优动作价值 (Q^*)——“最优”到底在最大化什么

本节只讲一个点：**最优性的定义与语义**。我们先把“对象”讲清楚：什么是 (V^*(s))、(Q^*(s,a))、最优策略 (\pi^*)，以及这些定义之间的必然关系。推导“max 形式的贝尔曼最优方程”放到 7.2。

---

1. 严格写出最优状态价值与最优动作价值的定义：
   [
   V^*(s)=\max_\pi V^\pi(s),\qquad Q^*(s,a)=\max_\pi Q^\pi(s,a)
   ]
2. 明确“最大化”的对象是什么：在给定 MDP（(P,R,\gamma) 固定）下，对**策略集合**取最大。
3. 理解最优策略 (\pi^*) 的定义与存在性直觉（在有限 MDP 下通常存在）。
4. 理解两个重要且常用的关系（定义层面即可理解）：
   [
   V^*(s)=\max_a Q^*(s,a),\qquad \pi^*(s)\in\arg\max_a Q^*(s,a)
   ]

---

### 1) 最优状态价值 (V^*(s)) 的定义

你已经知道：
[
V^\pi(s)=\mathbb{E}_\pi[G_t\mid S_t=s]
]
“最优”就是在所有策略里挑一个让它最大：

[
\boxed{V^*(s)=\max_{\pi} V^\pi(s)}
]

语义：从状态 (s) 出发，如果你可以自由选择策略（即未来所有状态都可以选择怎样行动），那么能达到的最大期望回报是多少。

---

### 2) 最优动作价值 (Q^*(s,a)) 的定义

同理：
[
Q^\pi(s,a)=\mathbb{E}_\pi[G_t\mid S_t=s,A_t=a]
]
最优动作价值定义为：

[
\boxed{Q^*(s,a)=\max_{\pi} Q^\pi(s,a)}
]

语义：在 (s) 先固定执行动作 (a)，之后你仍然可以选择“最好的策略”继续走，那么你能达到的最大期望回报是多少。

注意这里的“之后仍可选择最好的策略继续”非常关键：这正是最优性递推结构的来源（7.2 会推导）。

---

### 3) 最优策略 (\pi^*) 的定义

最优策略是使价值达到最优的策略：

[
\pi^*\in\arg\max_\pi V^\pi(s)\quad \text{对所有 } s
]

更精确的表述是：存在某个策略 (\pi^*)，使得对所有状态 (s)：
[
V^{\pi^*}(s)=V^*(s)
]
在有限 MDP（有限状态、有限动作、(\gamma<1)）里，通常存在最优确定性平稳策略（这是经典结论；我们后面会用直觉和结构解释它）。

---

### 4) 两个“定义层面即可理解”的关键关系

#### (A) (V^*(s)=\max_a Q^*(s,a))

理由（直觉但严谨）：

* (Q^*(s,a)) 表示“在 (s) 先做 (a)，之后再用最优方式继续”的最优期望回报。
* 如果你在 (s) 可以自由选择第一步动作，那么你显然会选使这个量最大的动作。

所以：
[
\boxed{V^*(s)=\max_a Q^*(s,a)}
]

#### (B) 最优策略可以由 (Q^*) 导出

如果你知道 (Q^*)，那么在每个状态选取最大 (Q^*) 的动作就是最优策略（允许并列）：
[
\boxed{\pi^*(s)\in\arg\max_a Q^*(s,a)}
]

这两条关系是后续 Q-learning、DQN 等算法的“目标形式”基础。


---

# 第 7 课（7.2）：贝尔曼最优方程为什么是 “max 形式”——从最优定义推导到可计算结构

本节只讲一个点：**从 (V^*(s)=\max_\pi V^\pi(s)) 的定义出发，严格解释为什么可以得到**
[
\boxed{
V^*(s)=\max_a \sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
}
]
并用代码实现对应的动态规划算法：**Value Iteration（价值迭代）**。

---

1. 从定义层面理解“最优子结构”（principle of optimality）：**最优策略的后缀仍然最优**。
2. 从 (G_t = R_{t+1} + \gamma G_{t+1}) 推出 (V^*) 的 max 递推形式。
3. 明确一件关键事实：**max 的对象不是随机变量本身，而是“你在当前状态可自由选择的第一步动作/后续策略”**。

---

### Step 0：回忆两个定义（不省略）

[
V^*(s)=\max_{\pi} V^\pi(s)=\max_{\pi}\mathbb{E}*\pi[G_t\mid S_t=s]
]
[
Q^*(s,a)=\max*{\pi} Q^\pi(s,a)=\max_{\pi}\mathbb{E}_\pi[G_t\mid S_t=s,A_t=a]
]

---

### Step 1：把“最优”拆成“先选第一步动作，再做后续最优”

在状态 (s)，你第一步可以选择动作 (a)。一旦你选了 (a)，接下来会到某个随机的 (S_{t+1})，之后你仍然可以“选择最优策略继续”。

因此，一个非常关键的语义等价是：
[
\boxed{V^*(s)=\max_a Q^*(s,a)}
]
这在 7.1 你已经用枚举验证过，它本质上只是“第一步动作也在你可控范围内”。

---

### Step 2：对 (Q^*(s,a)) 做“一步展开”

从回报递推：
[
G_t = R_{t+1}+\gamma G_{t+1}
]
代入 (Q^*) 的定义（注意：这里第一步动作固定为 (a)）：
[
Q^*(s,a)=\max_{\pi}\mathbb{E}*\pi[R*{t+1}+\gamma G_{t+1}\mid S_t=s,A_t=a]
]

现在关键来了：**在 (t+1) 之后，你可以重新选择最优策略**。给定下一状态 (S_{t+1}=s')，从那里开始“能拿到的最大期望回报”就是 (V^*(s'))（这就是“最优子结构/最优性原理”）。

所以在语义上：
[
\max_{\pi}\mathbb{E}*\pi[G*{t+1}\mid S_{t+1}=s'] = V^*(s')
]

于是：
[
\boxed{
Q^*(s,a)=\sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
}
]
（若奖励带噪声，把 (R) 换成条件期望奖励 (r)，形式不变。）

---

### Step 3：代回 (V^*(s)=\max_a Q^*(s,a))，得到贝尔曼最优方程

[
V^*(s)=\max_a \sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
]

这条方程在说什么（非常具体）：

* 你在 (s) 做一次“一步展望”：对每个动作 (a) 计算其**一步期望回报 + 折扣后继最优价值**
* 选择其中最大的动作
* 这就是最优价值

这不是“把 max 硬塞进去”，而是因为：**最优控制允许你在每个状态重新做最优决策**，后续问题结构与原问题同型（递推）。

---



# 第 7 课（7.3）：策略改进定理（Policy Improvement Theorem）——为什么“对 (V^\pi) 贪心”一定不更差

本节只讲一个点：**策略改进定理的核心不等式**。它解释了一个极其关键的事实：只要你对当前策略 (\pi) 的价值函数做“一步贪心改进”，得到的新策略 (\pi') 就不会更差，通常会更好。这是 Policy Iteration、Actor-Critic、以及很多“改进策略”的理论支点。

---

1. 正确陈述策略改进定理（核心结论）：
   若对任意状态 (s)，(\pi'(s)) 选择使 (Q^\pi(s,a)) 最大的动作，则
   [
   V^{\pi'}(s)\ge V^\pi(s)\quad \forall s
   ]
2. 理解证明只依赖一个“关键不等式链”：
   [
   Q^\pi(s,\pi'(s))\ge V^\pi(s)\ \Rightarrow\ V^{\pi'}(s)\ge V^\pi(s)
   ]
3. 明确“为什么只做一步贪心就能保证整体改进”：因为价值满足递推结构（贝尔曼期望方程）。

---

### 1) 先把定理说清楚（最常用的确定性版本）

给定任意策略 (\pi)，定义它的动作价值函数 (Q^\pi(s,a))。

构造一个新策略 (\pi')（确定性贪心）：
[
\pi'(s)\in\arg\max_{a} Q^\pi(s,a)
]

那么策略改进定理断言：
[
\boxed{
V^{\pi'}(s)\ge V^\pi(s)\quad \forall s
}
]
并且若存在某个状态严格改进（且能被达到），通常会得到严格不等式（本节不展开严格条件）。

---

### 2) 证明的第一步：贪心意味着“一步不等式”

由 (\pi') 的构造方式，对任意状态 (s)：
[
Q^\pi(s,\pi'(s)) ;=; \max_a Q^\pi(s,a)
]
又因为
[
V^\pi(s)=\sum_a \pi(a|s)Q^\pi(s,a)
]
一个最大值一定不小于按任何分布的加权平均，因此：
[
\boxed{
Q^\pi(s,\pi'(s)) \ge V^\pi(s)
}
]

这一步在做什么？
它把“对 (Q^\pi) 贪心”转换成一个可用的不等式：**在 (s) 用 (\pi'(s)) 的那一步，至少不会比按 (\pi) 的平均动作更差**。

---

### 3) 证明的第二步：把“一步不等式”递推展开到整个轨迹

注意一个重要恒等式（来自 (Q^\pi) 的定义/贝尔曼期望形式）：
[
Q^\pi(s,a)=\mathbb{E}\left[R_{t+1}+\gamma V^\pi(S_{t+1})\mid S_t=s,A_t=a\right]
]
因此对 (a=\pi'(s))：
[
Q^\pi(s,\pi'(s))
=\mathbb{E}\left[R_{t+1}+\gamma V^\pi(S_{t+1})\mid S_t=s,A_t=\pi'(s)\right]
]

结合上一步的不等式：
[
V^\pi(s)\le \mathbb{E}\left[R_{t+1}+\gamma V^\pi(S_{t+1})\mid S_t=s, A_t=\pi'(s)\right]
]
这句话的含义：**如果第一步按 (\pi') 做，那么“一步奖励 + 折扣后继 (V^\pi)”的期望至少不小于原来的 (V^\pi(s))**。

接下来关键是递推（直觉：把 (V^\pi(S_{t+1})) 再用同样方式上界）：

* 在 (S_{t+1}) 处，同样有
  [
  V^\pi(S_{t+1}) \le Q^\pi(S_{t+1},\pi'(S_{t+1}))
  ]
* 把它代入上面的右侧，就得到对“两步按 (\pi')”的上界
* 如此反复，就会得到：
  [
  V^\pi(s)\le \mathbb{E}*{\pi'}\left[\sum*{k=0}^{\infty}\gamma^k R_{t+k+1}\mid S_t=s\right] = V^{\pi'}(s)
  ]

因此：
[
\boxed{V^{\pi'}(s)\ge V^\pi(s)}
]

这一步在做什么、为什么这么做？

* 我们用 (G_t) 的递推结构，把“一步更好”推广到“整条轨迹不更差”。
* 这就是动态规划中“最优子结构/递推”产生威力的地方。

---


# 第 7 课（7.4）：Policy Iteration（策略迭代）为什么会收敛到最优策略——机制与完整实现

本节只讲一个点：**把 7.3 的“改进不更差”串成一个循环：评估 → 改进 → 评估 → …，并说明为什么在有限 MDP 中它会在有限步内停到最优（或某个最优之一）**。然后用代码实现完整 Policy Iteration。

---
1. 清晰描述 Policy Iteration 两步循环：

   * Policy Evaluation：求 (V^{\pi_k})
   * Policy Improvement：(\pi_{k+1}(s)\in\arg\max_a Q^{\pi_k}(s,a))
2. 理解“必然收敛”的两个原因：

   * **单调改进**：(V^{\pi_{k+1}}(s)\ge V^{\pi_k}(s);\forall s)
   * **策略数有限**（有限状态/动作，确定性平稳策略数有限），因此不可能无限严格提升
3. 知道它停下来的条件：(\pi_{k+1}=\pi_k) 时，策略已经是最优（满足最优性条件）。

---

### 1) 算法结构（你需要背下来的一句话）

给定初始策略 (\pi_0)，重复：

1. **评估**：用贝尔曼期望方程求 (V^{\pi_k})
2. **改进**：对 (Q^{\pi_k}) 贪心，得到 (\pi_{k+1})

直到策略不再变化。

---

### 2) 为什么它不会越改越差（核心：策略改进定理）

第 7.3 已证明：
[
\pi_{k+1}(s)\in\arg\max_a Q^{\pi_k}(s,a)\ \Rightarrow\ V^{\pi_{k+1}}(s)\ge V^{\pi_k}(s),\ \forall s
]
因此 (V^{\pi_k}) 是单调不减序列（逐轮不差，通常更好）。

---

### 3) 为什么它会停下来（核心：策略集合有限）

在有限 MDP 中，确定性平稳策略总数为：
[
|\Pi| = |\mathcal{A}|^{|\mathcal{S}_{nonterminal}|}
]
有限个。每次严格改进都会换一个策略，而价值不会无限严格上升，所以**必然在有限步内停下**。

---

### 4) 停下来的策略为什么是最优的

当 (\pi_{k+1}=\pi_k)，表示对所有状态 (s)：
[
\pi_k(s)\in\arg\max_a Q^{\pi_k}(s,a)
]
也就是“对自己的 (Q) 已经贪心”。这等价于满足贝尔曼最优性条件（本质上是最优方程的策略形式），因此该策略就是最优策略（或某个最优之一）。

---
