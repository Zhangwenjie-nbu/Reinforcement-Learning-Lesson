# 第 7 课（7.1）：最优价值 (V^*) 与最优动作价值 (Q^*)——“最优”到底在最大化什么

本节只讲一个点：**最优性的定义与语义**。我们先把“对象”讲清楚：什么是 (V^*(s))、(Q^*(s,a))、最优策略 (\pi^*)，以及这些定义之间的必然关系。推导“max 形式的贝尔曼最优方程”放到 7.2。

---

1. 严格写出最优状态价值与最优动作价值的定义：
   [
   V^*(s)=\max_\pi V^\pi(s),\qquad Q^*(s,a)=\max_\pi Q^\pi(s,a)
   ]
2. 明确“最大化”的对象是什么：在给定 MDP（(P,R,\gamma) 固定）下，对**策略集合**取最大。
3. 理解最优策略 (\pi^*) 的定义与存在性直觉（在有限 MDP 下通常存在）。
4. 理解两个重要且常用的关系（定义层面即可理解）：
   [
   V^*(s)=\max_a Q^*(s,a),\qquad \pi^*(s)\in\arg\max_a Q^*(s,a)
   ]

---

### 1) 最优状态价值 (V^*(s)) 的定义

你已经知道：
[
V^\pi(s)=\mathbb{E}_\pi[G_t\mid S_t=s]
]
“最优”就是在所有策略里挑一个让它最大：

[
\boxed{V^*(s)=\max_{\pi} V^\pi(s)}
]

语义：从状态 (s) 出发，如果你可以自由选择策略（即未来所有状态都可以选择怎样行动），那么能达到的最大期望回报是多少。

---

### 2) 最优动作价值 (Q^*(s,a)) 的定义

同理：
[
Q^\pi(s,a)=\mathbb{E}_\pi[G_t\mid S_t=s,A_t=a]
]
最优动作价值定义为：

[
\boxed{Q^*(s,a)=\max_{\pi} Q^\pi(s,a)}
]

语义：在 (s) 先固定执行动作 (a)，之后你仍然可以选择“最好的策略”继续走，那么你能达到的最大期望回报是多少。

注意这里的“之后仍可选择最好的策略继续”非常关键：这正是最优性递推结构的来源（7.2 会推导）。

---

### 3) 最优策略 (\pi^*) 的定义

最优策略是使价值达到最优的策略：

[
\pi^*\in\arg\max_\pi V^\pi(s)\quad \text{对所有 } s
]

更精确的表述是：存在某个策略 (\pi^*)，使得对所有状态 (s)：
[
V^{\pi^*}(s)=V^*(s)
]
在有限 MDP（有限状态、有限动作、(\gamma<1)）里，通常存在最优确定性平稳策略（这是经典结论；我们后面会用直觉和结构解释它）。

---

### 4) 两个“定义层面即可理解”的关键关系

#### (A) (V^*(s)=\max_a Q^*(s,a))

理由（直觉但严谨）：

* (Q^*(s,a)) 表示“在 (s) 先做 (a)，之后再用最优方式继续”的最优期望回报。
* 如果你在 (s) 可以自由选择第一步动作，那么你显然会选使这个量最大的动作。

所以：
[
\boxed{V^*(s)=\max_a Q^*(s,a)}
]

#### (B) 最优策略可以由 (Q^*) 导出

如果你知道 (Q^*)，那么在每个状态选取最大 (Q^*) 的动作就是最优策略（允许并列）：
[
\boxed{\pi^*(s)\in\arg\max_a Q^*(s,a)}
]

这两条关系是后续 Q-learning、DQN 等算法的“目标形式”基础。

---

## 微课 7.1（编码）：在一个小 MDP 上“枚举策略”算出 (V^*) 与 (Q^*)（用最原始方式验证定义）

为了把定义落地，我们用一个**非常小**的 MDP（状态 0..4，终止态 0 和 4，动作 LEFT/RIGHT），并做一件“最笨但最清楚”的事：

1. 枚举所有确定性平稳策略（因为状态少，策略总数有限）
2. 对每个策略用 6.1 的策略评估算 (V^\pi)
3. 对每个状态取最大，得到 (V^*)
4. 再用“第一步固定动作 + 之后按最优策略”的方式，构造出 (Q^*)（在本节里用枚举定义来做，不用贝尔曼最优方程）

你会亲眼看到：

* (V^*(s)) 确实是所有策略中最大的
* (V^*(s)=\max_a Q^*(s,a)) 数值上成立
* 最优策略在每个状态就是选右走（在这个任务设定下）

---

# 第 7 课（7.2）：贝尔曼最优方程为什么是 “max 形式”——从最优定义推导到可计算结构

本节只讲一个点：**从 (V^*(s)=\max_\pi V^\pi(s)) 的定义出发，严格解释为什么可以得到**
[
\boxed{
V^*(s)=\max_a \sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
}
]
并用代码实现对应的动态规划算法：**Value Iteration（价值迭代）**。

---

1. 从定义层面理解“最优子结构”（principle of optimality）：**最优策略的后缀仍然最优**。
2. 从 (G_t = R_{t+1} + \gamma G_{t+1}) 推出 (V^*) 的 max 递推形式。
3. 明确一件关键事实：**max 的对象不是随机变量本身，而是“你在当前状态可自由选择的第一步动作/后续策略”**。

---

### Step 0：回忆两个定义（不省略）

[
V^*(s)=\max_{\pi} V^\pi(s)=\max_{\pi}\mathbb{E}*\pi[G_t\mid S_t=s]
]
[
Q^*(s,a)=\max*{\pi} Q^\pi(s,a)=\max_{\pi}\mathbb{E}_\pi[G_t\mid S_t=s,A_t=a]
]

---

### Step 1：把“最优”拆成“先选第一步动作，再做后续最优”

在状态 (s)，你第一步可以选择动作 (a)。一旦你选了 (a)，接下来会到某个随机的 (S_{t+1})，之后你仍然可以“选择最优策略继续”。

因此，一个非常关键的语义等价是：
[
\boxed{V^*(s)=\max_a Q^*(s,a)}
]
这在 7.1 你已经用枚举验证过，它本质上只是“第一步动作也在你可控范围内”。

---

### Step 2：对 (Q^*(s,a)) 做“一步展开”

从回报递推：
[
G_t = R_{t+1}+\gamma G_{t+1}
]
代入 (Q^*) 的定义（注意：这里第一步动作固定为 (a)）：
[
Q^*(s,a)=\max_{\pi}\mathbb{E}*\pi[R*{t+1}+\gamma G_{t+1}\mid S_t=s,A_t=a]
]

现在关键来了：**在 (t+1) 之后，你可以重新选择最优策略**。给定下一状态 (S_{t+1}=s')，从那里开始“能拿到的最大期望回报”就是 (V^*(s'))（这就是“最优子结构/最优性原理”）。

所以在语义上：
[
\max_{\pi}\mathbb{E}*\pi[G*{t+1}\mid S_{t+1}=s'] = V^*(s')
]

于是：
[
\boxed{
Q^*(s,a)=\sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
}
]
（若奖励带噪声，把 (R) 换成条件期望奖励 (r)，形式不变。）

---

### Step 3：代回 (V^*(s)=\max_a Q^*(s,a))，得到贝尔曼最优方程

[
V^*(s)=\max_a \sum_{s'}P(s'|s,a)\left(R(s,a,s')+\gamma V^*(s')\right)
]

这条方程在说什么（非常具体）：

* 你在 (s) 做一次“一步展望”：对每个动作 (a) 计算其**一步期望回报 + 折扣后继最优价值**
* 选择其中最大的动作
* 这就是最优价值

这不是“把 max 硬塞进去”，而是因为：**最优控制允许你在每个状态重新做最优决策**，后续问题结构与原问题同型（递推）。

---
