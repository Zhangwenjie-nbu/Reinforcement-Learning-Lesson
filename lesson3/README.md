  # 第 3 课（3.1）：奖励 (R) 的本质——“规则/分布” vs “一次采样值”

  * **奖励机制（reward model / reward kernel）**：环境“如何给奖励”的规则或概率分布
  * **奖励样本（reward sample）**：某一次交互实际拿到的 (R_{t+1})
  2. 明白为什么“未来奖励无法逐步算出”，但“期望回报/价值函数”仍可定义并可学习。
  3. 理解三个常见定义口径之间的关系：

  * (R(s,a,s'))（确定性函数）
  * (R_{t+1})（随机变量的实现）
  * (r(s,a)=\mathbb{E}[R_{t+1}\mid S_t=s,A_t=a])（期望即时奖励）

  ---

  ### 1 先统一随机变量记号

  在交互过程中，环境产生随机序列：
  [
  S_t,\ A_t,\ R_{t+1},\ S_{t+1},\ldots
  ]
  其中：

  * (S_t)：时刻 (t) 的状态随机变量（一次 episode 中会取到具体值）
  * (A_t)：你在时刻 (t) 的动作随机变量（由策略采样得到）
  * (R_{t+1})：你执行 (A_t) 后**环境给出的奖励随机变量**
  * (S_{t+1})：下一状态随机变量（由转移核采样）

  关键：**(R_{t+1}) 是随机变量**，你每次运行环境都会得到不同的样本值。

  ---

  ### 2 “奖励机制”有两种常见写法（容易混淆）

  在文献里你会看到两种口径：

  **口径 A：奖励是确定性函数**
  [
  R(s,a,s') \in \mathbb{R}
  ]
  意思是：一旦发生了 (s \xrightarrow{a} s')，奖励就是固定值。
  注意：即使 (R(s,a,s')) 是确定的，只要 (S_{t+1}) 是随机的，那么从 ((s,a)) 出发的奖励仍然是随机的（因为你不知道会落到哪个 (s')）。

  **口径 B：奖励本身也可能随机**（更一般）
  [
  p(r \mid s,a,s')
  ]
  意思是：即便转移到了同一个 (s')，奖励还可能带噪声（例如传感器噪声、随机对手、随机事件）。
  这时 (R_{t+1}) 的随机性来源有两层：转移随机性 + 奖励噪声。

  实践中，你通常只拿到：给定 ((s,a))（或 ((s,a,s'))），环境返回一个样本 (r)。

  ---

  ### 3 “能计算的”不是未来每一步奖励，而是条件期望

  你提出的困惑非常到点子上：未来动作、未来状态都不确定，怎么计算未来奖励？

  答案是：**我们不计算“确定的未来奖励序列”，而计算它的期望（或用采样近似这个期望）。**

  常用两个期望对象：

  **(1) 条件在 ((s,a)) 的期望即时奖励**
  [
  r(s,a)=\mathbb{E}[R_{t+1}\mid S_t=s,A_t=a]
  ]
  展开后（离散状态）：
  [
  r(s,a)=\sum_{s'}P(s'|s,a)\ \mathbb{E}[R_{t+1}\mid s,a,s']
  ]

  **(2) 条件在 ((s,a,s')) 的期望奖励**
  [
  r(s,a,s')=\mathbb{E}[R_{t+1}\mid S_t=s,A_t=a,S_{t+1}=s']
  ]
  如果奖励是确定性函数 (R(s,a,s'))，那它就等于 (R(s,a,s'))。

  你看见了吗：不确定性被“装进了期望符号 (\mathbb{E}[\cdot])”里；这正是 RL 可学习的根基。

  # 第 3 课（3.2）：如何稳定估计 (r(s,a)=\mathbb{E}[R_{t+1}\mid s,a])（方差、样本量、置信区间直觉）


  1. 写出并解释最常用的估计器：样本均值
  [
  \hat r(s,a)=\frac{1}{N}\sum_{i=1}^{N} r_i
  ]
  2. 明白估计误差来自哪里：奖励本身的随机性（方差）+ 样本量有限（(N) 小）。
  3. 形成可操作的判断：**标准误差 (\propto 1/\sqrt{N})**，所以想把误差缩小一半，样本量要扩大约 4 倍。
  4. 理解它与探索的关系：如果 ((s,a)) 很少被访问（覆盖低），那 (N(s,a)) 小，估计就不稳。

  ---

  ### 1 “你能估计的”是条件期望，而不是未来确定值

  对固定的 ((s,a))，环境给你的即时奖励是随机变量 (R_{t+1})。你关心的是：
  [
  r(s,a)=\mathbb{E}[R_{t+1}\mid S_t=s,A_t=a]
  ]
  但你拿到的是样本：(r_1,r_2,\ldots,r_N)。

  ---

  ### 2 最常用估计器：样本均值（在线/离线都一样）

  [
  \hat r(s,a)=\frac{1}{N}\sum_{i=1}^{N} r_i
  ]
  性质（直观层面够用）：

  * **无偏**：平均而言 (\mathbb{E}[\hat r]=r(s,a))（在样本独立同分布的前提下）
  * **方差随样本量下降**：
  [
  \mathrm{Var}(\hat r)=\frac{\mathrm{Var}(R_{t+1}\mid s,a)}{N}
  ]

  这直接带来一个核心尺度律：

  * 估计的不确定性（标准差/标准误差）大致是：
  [
  \text{SE} \approx \frac{\sigma}{\sqrt{N}}
  ]
  (\sigma) 是奖励分布的标准差（未知，可用样本方差估）。

  ---

  ### 3 “需要多少数据”——用置信区间给直觉

  在 (N) 足够大时（常见做法），可以用近似的 95% 置信区间：
  [
  \hat r \pm 1.96 \cdot \frac{\hat\sigma}{\sqrt{N}}
  ]
  含义（工程直觉）：

  * 不只得到一个点估计 (\hat r)，还得到一个“可信范围”。
  * (N) 越大，区间越窄；噪声越大（(\sigma) 越大），区间越宽。

  ---

  ### 4 为什么这与探索强相关（非常关键）

  估计稳定性的瓶颈不是“总交互步数”，而是 **(N(s,a))**：

  * 如果策略几乎不选某个动作，那么该动作对应的 (N(s,a)) 很小，(\hat r(s,a)) 的误差就大。
  * 这就是探索的统计学根源：**你需要数据覆盖，才能可靠比较动作优劣。**

  # 第 3 课（3.3）：稀疏奖励为什么难学，以及“学习信号密度”如何影响方差与样本效率

  1. 精确定义“稀疏奖励”的含义：奖励事件发生概率低、或大部分时间步奖励为 0。
  2. 从统计角度理解“难学”的根因：**方差大 + 有效样本少**。
  3. 理解密集/塑形奖励的作用：不是“更聪明的算法”，而是**更频繁的可用监督信号**。

  ---

  ### 1 稀疏奖励的本质：大部分样本没有信息

  以最典型的设定为例：只有到达目标才给 +1，否则 0。
  那么对很多策略（尤其是随机/初期策略），你会观察到：

  * 大多数 episode 的回报 (G) 为 0（没到达目标）
  * 少数 episode 为正（到达目标）

  这导致两个后果：

  **(A) 有效样本比例极低**
  如果成功概率是 (p)，那么平均要 (\approx 1/p) 条 episode 才能看到一次正回报。

  **(B) 估计方差大，收敛慢**
  对“是否成功”的伯努利事件而言，方差是 (p(1-p))。当 (p) 很小，虽然单次方差上界不高，但**你很久都看不到正样本**，导致对“哪个动作更好”的比较极不稳定（覆盖不足、差异被噪声淹没）。

  更一般地，在 RL 中这会表现为：

  * **信用分配（credit assignment）困难**：奖励在很后面才出现，无法直接告诉你哪些早期动作贡献了成功。
  * **学习信号延迟**：大部分时间步的 TD target 只有 bootstrapping 项，真实 reward 信息几乎没有。

  ---

  ### 2 密集奖励/奖励塑形的作用：提高“信号密度”

  密集奖励不一定要改变任务目标，它的核心价值在于：

  * 让更多时间步产生非零、方向性明确的反馈（例如“往目标方向走得到小正反馈”）。
  * 使得在相同样本量下，估计的方差更小，策略改进更稳定。

  一个重要原则（只点到为止，后面会专门展开）：

  * **Potential-based shaping**：用势函数 (\phi(s)) 构造塑形项
    [
    F(s,a,s')=\gamma\phi(s')-\phi(s)
    ]
    在一定条件下可以**不改变最优策略**，但显著提升学习信号密度。

