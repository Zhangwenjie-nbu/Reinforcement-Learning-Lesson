# 第 5 课（5.1）：策略 (\pi(a|s)) 到底是什么对象——“从状态到动作分布的映射”

1. 明确策略的定义：(\pi(a|s)) 是一个**条件概率分布**（或密度）。
2. 区分两类策略：**随机策略（stochastic policy）** vs **确定性策略（deterministic policy）**。
3. 理解为什么 RL 中“策略必须可采样（sampleable）”：交互数据来自采样而非解析求积分。
4. 明确策略与探索的关系：探索可以被写进策略分布（例如 ε-greedy 是一种随机策略）。

---

### 1 策略的严格定义

在离散动作空间里，策略是：
[
\pi(\cdot|s): \mathcal{A} \to [0,1], \qquad \sum_{a\in\mathcal{A}} \pi(a|s)=1
]
也就是说：对每个状态 (s)，策略给出一个“在动作集合上的概率分布”。

因此 (\pi) 是一个“函数”，输入是 (s)，输出是一整个分布。

---

### 2 随机策略 vs 确定性策略

**随机策略**：

* 对同一 (s)，动作是随机的，按 (\pi(a|s)) 采样。
* 例如 ε-greedy：以 (1-\varepsilon) 概率选贪心动作，以 (\varepsilon) 概率随机选动作。

**确定性策略**：

* 直接给出一个动作：
  [
  a = \mu(s)
  ]
* 也可以看作随机策略的特殊情形：所有概率质量集中在一个动作上（delta 分布）。

工程含义：

* 随机策略天然包含探索、可表示不确定性；
* 确定性策略在连续控制里常用（例如 DDPG/TD3 方向），但训练时仍常加入噪声做探索。

---

### 3 “策略必须可采样”是什么意思

你和环境交互得到数据的方式是：
[
A_t \sim \pi(\cdot|S_t)
]
也就是说，策略不仅要能“算出概率”，还要能“给我一个动作样本”。这在实现上非常关键：你需要写出 `act(s)` 函数。

