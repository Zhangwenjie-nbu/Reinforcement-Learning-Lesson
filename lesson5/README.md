# 第 5 课（5.1）：策略 (\pi(a|s)) 到底是什么对象——“从状态到动作分布的映射”

1. 明确策略的定义：(\pi(a|s)) 是一个**条件概率分布**（或密度）。
2. 区分两类策略：**随机策略（stochastic policy）** vs **确定性策略（deterministic policy）**。
3. 理解为什么 RL 中“策略必须可采样（sampleable）”：交互数据来自采样而非解析求积分。
4. 明确策略与探索的关系：探索可以被写进策略分布（例如 ε-greedy 是一种随机策略）。

---

### 1 策略的严格定义

在离散动作空间里，策略是：
[
\pi(\cdot|s): \mathcal{A} \to [0,1], \qquad \sum_{a\in\mathcal{A}} \pi(a|s)=1
]
也就是说：对每个状态 (s)，策略给出一个“在动作集合上的概率分布”。

因此 (\pi) 是一个“函数”，输入是 (s)，输出是一整个分布。

---

### 2 随机策略 vs 确定性策略

**随机策略**：

* 对同一 (s)，动作是随机的，按 (\pi(a|s)) 采样。
* 例如 ε-greedy：以 (1-\varepsilon) 概率选贪心动作，以 (\varepsilon) 概率随机选动作。

**确定性策略**：

* 直接给出一个动作：
  [
  a = \mu(s)
  ]
* 也可以看作随机策略的特殊情形：所有概率质量集中在一个动作上（delta 分布）。

工程含义：

* 随机策略天然包含探索、可表示不确定性；
* 确定性策略在连续控制里常用（例如 DDPG/TD3 方向），但训练时仍常加入噪声做探索。

---

### 3 “策略必须可采样”是什么意思

你和环境交互得到数据的方式是：
[
A_t \sim \pi(\cdot|S_t)
]
也就是说，策略不仅要能“算出概率”，还要能“给我一个动作样本”。这在实现上非常关键：你需要写出 `act(s)` 函数。

# 第 5 课（5.2）：状态价值 (V^\pi(s)) 的定义与“期望到底对谁取”

本节只讲一个点：**(V^\pi(s)) 的严格定义、语义，以及它为什么“不可直接观测，只能估计”。**这会直接决定你后面为什么需要 MC/TD，以及为什么“采样”是 RL 的基本工作方式。


1. 写出并解释 (V^\pi(s)) 的定义：
   [
   V^\pi(s)=\mathbb{E}_\pi[G_t \mid S_t=s]
   ]
2. 明确期望 (\mathbb{E}_\pi[\cdot]) 到底对哪些随机性取：

   * 策略的随机性 (A_t \sim \pi(\cdot|S_t))
   * 环境的随机性 (S_{t+1}\sim P(\cdot|S_t,A_t))
     -（可选）奖励噪声 (R_{t+1}) 也可能随机
3. 理解：为什么 (V^\pi(s)) 不是一次交互就能“读出来”的量，而是一个**分布的期望**，只能通过多次采样估计。

---

### 1 定义：价值就是“从这里出发，按策略走，长期平均能拿多少”

我们已经定义回报：
[
G_t=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
]
然后价值函数定义为：
[
V^\pi(s)=\mathbb{E}[G_t \mid S_t=s,\ A_{t:\infty}\sim\pi,\ S_{t+1:\infty}\sim P]
]

直观翻译（重要）：

* “我现在在 (s)。”
* “接下来每一步都按策略 (\pi) 选动作。”
* “环境按它的随机转移规则响应。”
* “那么最终长期折扣回报的**期望**是多少？”——这就是 (V^\pi(s))。

---

### 2 期望对谁取：两层（或三层）随机性

即使策略是确定性的，环境也可能随机；即使环境是确定性的，策略也可能随机。
因此 (G_t) 一般是随机变量，(V^\pi(s)) 是它的条件期望。

* **策略随机性**：同一状态下，动作可能不同
* **环境随机性**：同一 ((s,a)) 下，下一状态可能不同
* **奖励随机性**：同一 ((s,a,s')) 下奖励也可能带噪声

---

### 3 为什么 (V^\pi(s)) 不能直接观测

你一次从 (s) 出发跑完一条 episode，你得到的是一个样本回报 (G_t)。
但 (V^\pi(s)) 是“样本回报的平均值（期望）”。

因此：

* 单次 episode：只能得到 (G_t^{(1)})
* 多次 episode：得到 (G_t^{(1)},G_t^{(2)},\ldots)，然后用均值估计
  [
  \hat V^\pi(s)=\frac{1}{N}\sum_{i=1}^N G^{(i)}_t
  ]

这就是为什么后续 MC 方法自然出现：它就是在做这件事。


# 第 5 课（5.3）：动作价值 (Q^\pi(s,a)) 是什么，以及它与 (V^\pi(s)) 的关系

本节只讲一个点：**为什么需要 (Q^\pi(s,a))**，它的严格定义是什么，以及它与 (V^\pi(s)) 的一个核心关系：
[
V^\pi(s)=\sum_{a}\pi(a|s),Q^\pi(s,a)
]
本节不推贝尔曼方程，只讲定义与语义，把对象彻底讲清楚。


1. 写出 (Q^\pi(s,a)) 的定义：
   [
   Q^\pi(s,a)=\mathbb{E}_\pi[G_t \mid S_t=s, A_t=a]
   ]
2. 解释它的语义：**在 (s) 先强制做 (a)，之后再按 (\pi) 行为**时的期望回报。
3. 理解为什么控制问题更喜欢 (Q)：因为“决策”在动作层面发生。
4. 理解与 (V^\pi) 的关系式为什么成立（按全概率/对动作取期望的直觉）。

---

### 1 定义：比 (V^\pi(s)) 多固定了一个“第一步动作”

* (V^\pi(s))：只给定起点状态 (s)，动作由策略随机选。
* (Q^\pi(s,a))：给定起点状态 (s)，并且第一步动作固定为 (a)，之后动作才由 (\pi) 决定。

形式化：
[
Q^\pi(s,a)=\mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\ \bigg|\ S_t=s, A_t=a,\ A_{t+1:\infty}\sim \pi \right]
]

---

### 2 为什么控制更喜欢 (Q)

控制的核心是：在状态 (s) 下你要挑哪个动作。
而 (Q^\pi(s,a)) 直接回答：“如果我现在选 (a)，长期平均能拿多少？”
因此：

* 行为选择（control）通常围绕 (Q) 做 argmax 或 softmax。
* 价值 (V) 更多是“状态本身好不好”，但不能直接告诉你“该选哪个动作”。

---

### 3 核心关系：(V) 是对 (Q) 的动作期望

如果你在 (s) 按策略选动作 (A\sim \pi(\cdot|s))，那么：
[
V^\pi(s)=\mathbb{E}[G_t\mid S_t=s]
]
对动作进行条件展开（直觉：先“抽一个动作”，再看该动作的期望回报）：
[
V^\pi(s)=\sum_a P(A_t=a\mid S_t=s)\ \mathbb{E}[G_t\mid S_t=s,A_t=a]
=\sum_a \pi(a|s)\ Q^\pi(s,a)
]
这不是贝尔曼，这是**全概率/条件期望分解**。

