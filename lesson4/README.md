# 第 4 课（4.1）：回报 (G_t) 的递推形式为什么成立，以及它在后续算法中的作用

1. 从回报的求和定义严格推导出递推式 (G_t = R_{t+1} + \gamma G_{t+1})。
2. 说清楚终止时 (G_{T}=0)（或吸收态后为 0）在递推里扮演的角色。
3. 理解这条递推为贝尔曼方程提供了什么“可分解结构”。

---

### 1 回报的求和定义（先固定为有限 episode，最清晰）

设一条 episode 在时间 (T) 结束（到达 terminal），则对任意 (t<T)：
[
G_t ;=; \sum_{k=0}^{T-t-1} \gamma^k , R_{t+k+1}
]
这里要点是：**(G_t) 是随机变量**（因为未来的 (R) 由未来状态/动作决定）。

---

### 2 递推式的严格推导（只做“拆第一项”）

把求和的第一项单独拿出来：

[
\begin{aligned}
G_t
&= R_{t+1} + \sum_{k=1}^{T-t-1} \gamma^k R_{t+k+1} \
&= R_{t+1} + \gamma \sum_{k=1}^{T-t-1} \gamma^{k-1} R_{t+k+1}
\end{aligned}
]

令 (j=k-1)，则当 (k=1\Rightarrow j=0)，当 (k=T-t-1\Rightarrow j=T-(t+1)-1)，于是：

[
\sum_{k=1}^{T-t-1} \gamma^{k-1} R_{t+k+1}
=\sum_{j=0}^{T-(t+1)-1} \gamma^j R_{(t+1)+j+1}
=G_{t+1}
]

所以得到：
[
G_t = R_{t+1} + \gamma G_{t+1}
]

这一步非常重要：**递推不是假设，是定义的代数重写**。

---

### 3 终止时为什么取 (G_T = 0)

当 episode 在 (T) 结束后，后续不再有奖励项可加，因此自然定义：
[
G_T = 0
]
这使得递推在最后一步闭合：
[
G_{T-1} = R_T + \gamma G_T = R_T
]

注意：如果你用“吸收态”建模终止，那么吸收态之后奖励设为 0，也会得到同样的结论。

---

### 4 递推结构的意义（为后续铺路）

有了
[
G_t = R_{t+1} + \gamma G_{t+1}
]
你就能把“长期回报”分解为：

* **一步可观测奖励** (R_{t+1})
* **下一时刻的同类问题** (G_{t+1})

这正是贝尔曼方程与 TD 学习的根：把“长链条”压缩成“当前一步 + 下一步的价值”。


# 第 4 课（4.2）：折扣因子 (\gamma) 的两层意义——偏好与收敛性（收缩直觉）

1. 从“权重”角度解释 (\gamma)：未来奖励如何被衰减。
2. 从“有效视野（effective horizon）”角度解释 (\gamma)：多远的未来对当前决策仍然重要。
3. 从“收敛/稳定”角度给出直觉：为什么 (\gamma<1) 时价值函数更容易有限且迭代更稳定。
4. 通过一个最小数值实验看到：(\gamma) 越接近 1，估计更依赖长序列、方差更大、收敛更慢。

---

### 1 偏好层：未来奖励的权重衰减

回报定义：
[
G_t=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
]

* (\gamma=0)：只看下一步奖励 (R_{t+1})（纯短视）。
* (\gamma) 越大：越重视未来（长期主义）。
* (\gamma\to 1)：未来很远的奖励仍然有显著权重。

---

### 2 有效视野：不是“无穷”，而是“权重衰减到可忽略”

常用工程直觉：当 (\gamma^k) 足够小（例如 < 0.01）时，(k) 之后的奖励对回报贡献很小。

定义一个阈值 (\epsilon)，解：
[
\gamma^k \le \epsilon \quad \Rightarrow\quad k \ge \frac{\ln(\epsilon)}{\ln(\gamma)}
]
这给你一个“有效视野”的量级：

* (\gamma=0.90)：视野较短
* (\gamma=0.99)：视野非常长（你需要考虑更远未来）

---

### 3 数学层：为什么 (\gamma<1) 带来“有限值”和“稳定迭代”的直觉

以 continuing task（无限步）为例，如果每步奖励上界为 (|R|\le R_{\max})，则：
[
|G_t|\le \sum_{k=0}^{\infty}\gamma^k R_{\max}=\frac{R_{\max}}{1-\gamma}
]
当 (\gamma<1) 时，这个上界是有限的；当 (\gamma=1) 时一般会发散（除非奖励结构非常特殊，如最终为 0 或平均回报形式）。

更重要的是后续贝尔曼相关迭代的直觉（不做严格证明）：

* 迭代更新中“未来部分”会被乘以 (\gamma)。
* (\gamma<1) 让“误差传播被缩小”，表现为一种“收缩”倾向，从而更容易收敛。
* (\gamma) 越接近 1，这种缩小越弱，收敛越慢、对噪声更敏感。

